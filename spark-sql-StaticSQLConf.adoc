== [[StaticSQLConf]] StaticSQLConf -- Cross-Session, Immutable and Static SQL Configuration

`StaticSQLConf` holds <<properties, cross-session, immutable and static SQL configuration properties>>.

[[properties]]
.StaticSQLConf's Configuration Properties
[cols="30m,70",options="header",width="100%"]
|===
| Name
| Description

| spark.sql.catalogImplementation
a| [[spark.sql.catalogImplementation]][[CATALOG_IMPLEMENTATION]] *(internal)* Sets the xref:spark-sql-ExternalCatalog.adoc[ExternalCatalog]:

* xref:spark-sql-ExternalCatalog.adoc#in-memory[in-memory] (default)
* xref:spark-sql-ExternalCatalog.adoc#hive[hive]

NOTE: Use link:spark-sql-SparkSession-Builder.adoc#enableHiveSupport[Builder.enableHiveSupport] to enable Hive support (that sets `spark.sql.catalogImplementation` configuration property to `hive` only when the Hive classes are available).

Used when:

* `SparkSession` is requested for the link:spark-sql-SparkSession.adoc#sessionState[SessionState]

* `SharedState` is requested for the link:spark-sql-SharedState.adoc#externalCatalogClassName[ExternalCatalog]

* `SetCommand` command is executed (with *hive* keys)

* `SparkSession` is link:spark-sql-SparkSession-Builder.adoc#enableHiveSupport[created with Hive support]

| spark.sql.debug
a| [[spark.sql.debug]][[DEBUG_MODE]] *(internal)* Only used for internal debugging when `HiveExternalCatalog` is requested to link:hive/HiveExternalCatalog.adoc#restoreTableMetadata[restoreTableMetadata].

Default: `false`

Not all functions are supported when enabled.

| spark.sql.extensions
a| [[spark.sql.extensions]][[SPARK_SESSION_EXTENSIONS]] Name of the *SQL extension configuration class* that is used to configure `SparkSession` extensions (when `Builder` is requested to <<spark-sql-SparkSession-Builder.adoc#getOrCreate, get or create a SparkSession>>). The class should implement `Function1[SparkSessionExtensions, Unit]`, and must have a no-args constructor.

Default: (empty)

| spark.sql.filesourceTableRelationCacheSize
a| [[spark.sql.filesourceTableRelationCacheSize]][[FILESOURCE_TABLE_RELATION_CACHE_SIZE]] *(internal)* The maximum size of the cache that maps qualified table names to table relation plans. Must not be negative.

Default: `1000`

| spark.sql.globalTempDatabase
a| [[spark.sql.globalTempDatabase]][[GLOBAL_TEMP_DATABASE]] *(internal)* Name of the Spark-owned internal database of global temporary views

Default: `global_temp`

Used exclusively to create a <<spark-sql-GlobalTempViewManager.adoc#creating-instance, GlobalTempViewManager>> when `SharedState` is first requested for the <<spark-sql-SharedState.adoc#globalTempViewManager, GlobalTempViewManager>>.

NOTE: The name of the internal database cannot conflict with the names of any database that is already available in <<spark-sql-SharedState.adoc#externalCatalog, ExternalCatalog>>.

| spark.sql.hive.thriftServer.singleSession
| [[spark.sql.hive.thriftServer.singleSession]][[HIVE_THRIFT_SERVER_SINGLESESSION]] When enabled (`true`), Hive Thrift server is running in a single session mode. All the JDBC/ODBC connections share the temporary views, function registries, SQL configuration and the current database.

Default: `false`

| spark.sql.queryExecutionListeners
a| [[spark.sql.queryExecutionListeners]][[QUERY_EXECUTION_LISTENERS]] List of class names that implement <<spark-sql-QueryExecutionListener.adoc#, QueryExecutionListener>> that will be automatically <<spark-sql-ExecutionListenerManager.adoc#register, registered>> to new `SparkSessions`.

Default: (empty)

The classes should have either a no-arg constructor, or a constructor that expects a `SparkConf` argument.

| spark.sql.sources.schemaStringLengthThreshold
a| [[spark.sql.sources.schemaStringLengthThreshold]][[SCHEMA_STRING_LENGTH_THRESHOLD]] *(internal)* The maximum length allowed in a single cell when storing additional schema information in Hive's metastore

Default: `4000`

| spark.sql.ui.retainedExecutions
a| [[spark.sql.ui.retainedExecutions]][[UI_RETAINED_EXECUTIONS]] Number of executions to retain in the Spark UI.

Default: `1000`

| spark.sql.warehouse.dir
a| [[spark.sql.warehouse.dir]][[WAREHOUSE_PATH]] Directory of a Hive warehouse (using Derby) with managed databases and tables (_Spark warehouse_)

Default: `spark-warehouse`

TIP: Read the official https://cwiki.apache.org/confluence/display/Hive/AdminManual+MetastoreAdmin[Hive Metastore Administration] document to learn more.

|===

The <<properties, properties>> in `StaticSQLConf` can only be queried and can never be changed once the first `SparkSession` is created.

[source, scala]
----
import org.apache.spark.sql.internal.StaticSQLConf
scala> val metastoreName = spark.conf.get(StaticSQLConf.CATALOG_IMPLEMENTATION.key)
metastoreName: String = hive

scala> spark.conf.set(StaticSQLConf.CATALOG_IMPLEMENTATION.key, "hive")
org.apache.spark.sql.AnalysisException: Cannot modify the value of a static config: spark.sql.catalogImplementation;
  at org.apache.spark.sql.RuntimeConfig.requireNonStaticConf(RuntimeConfig.scala:144)
  at org.apache.spark.sql.RuntimeConfig.set(RuntimeConfig.scala:41)
  ... 50 elided
----

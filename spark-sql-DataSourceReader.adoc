== [[DataSourceReader]] DataSourceReader Contract

`DataSourceReader` is the <<contract, abstraction>> of <<implementations, data source readers>> in <<spark-sql-data-source-api-v2.adoc#, Data Source API V2>> that can <<planInputPartitions, planInputPartitions>> and know the <<readSchema, schema for reading>>.

[[contract]]
.DataSourceReader Contract
[cols="1m,3",options="header",width="100%"]
|===
| Method
| Description

| planInputPartitions
a| [[planInputPartitions]]

[source, java]
----
List<InputPartition<InternalRow>> planInputPartitions()
----

Used exclusively when `DataSourceV2ScanExec` leaf physical operator is requested for the <<spark-sql-SparkPlan-DataSourceV2ScanExec.adoc#partitions, input partitions>> (and simply delegates to the underlying <<spark-sql-SparkPlan-DataSourceV2ScanExec.adoc#reader, DataSourceReader>>) to create the input `RDD[InternalRow]` (`inputRDD`)

| readSchema
a| [[readSchema]]

[source, java]
----
StructType readSchema()
----

<<spark-sql-StructType.adoc#, Schema>> to use for reading (loading) data from a data source

Used when:

* `DataSourceV2Relation` factory object is requested to <<spark-sql-LogicalPlan-DataSourceV2Relation.adoc#create, create a DataSourceV2Relation>> (when `DataFrameReader` is requested to <<spark-sql-DataFrameReader.adoc#load, "load" data (as a DataFrame)>> from a data source with <<spark-sql-ReadSupport.adoc#, ReadSupport>>)

* `DataSourceV2Strategy` execution planning strategy is requested to apply column pruning (`pruneColumns`)

* Spark Structured Streaming's `MicroBatchExecution` stream execution is requested to run a single streaming batch

* Spark Structured Streaming's `ContinuousExecution` stream execution is requested to run a streaming query in continuous mode

* Spark Structured Streaming's `DataStreamReader` is requested to "load" data (as a DataFrame)

|===

[NOTE]
====
`DataSourceReader` is an `Evolving` contract that is evolving towards becoming a stable API, but is not a stable API yet and can change from one feature release to another release.

In other words, using the contract is as _"treading on thin ice"_.
====

[[implementations]]
.DataSourceReaders (Direct Implementations)
[cols="1,3",options="header",width="100%"]
|===
| DataSourceReader
| Description

| ContinuousReader
| [[ContinuousReader]] DataSourceReaders for *Continuous Stream Processing* in Spark Structured Streaming

Consult https://jaceklaskowski.gitbooks.io/spark-structured-streaming/spark-sql-streaming-ContinuousReader.html[The Internals of Spark Structured Streaming]

| MicroBatchReader
| [[MicroBatchReader]] DataSourceReaders for *Micro-Batch Stream Processing* in Spark Structured Streaming

Consult https://jaceklaskowski.gitbooks.io/spark-structured-streaming/spark-sql-streaming-MicroBatchReader.html[The Internals of Spark Structured Streaming]

| <<spark-sql-SupportsPushDownFilters.adoc#, SupportsPushDownFilters>>
| [[SupportsPushDownFilters]]

| <<spark-sql-SupportsPushDownRequiredColumns.adoc#, SupportsPushDownRequiredColumns>>
| [[SupportsPushDownRequiredColumns]]

| <<spark-sql-SupportsReportPartitioning.adoc#, SupportsReportPartitioning>>
| [[SupportsReportPartitioning]]

| <<spark-sql-SupportsReportStatistics.adoc#, SupportsReportStatistics>>
| [[SupportsReportStatistics]]

| <<spark-sql-SupportsScanColumnarBatch.adoc#, SupportsScanColumnarBatch>>
| [[SupportsScanColumnarBatch]]

|===

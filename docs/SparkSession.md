# SparkSession &mdash; The Entry Point to Spark SQL

`SparkSession` is the entry point to Spark SQL. It is one of the very first objects created in a Spark SQL application.

`SparkSession` is [created](#creating-instance) using the [SparkSession.builder](#builder) method (that gives access to [Builder API](spark-sql-SparkSession-Builder.md) to configure the `SparkSession`).

```scala
import org.apache.spark.sql.SparkSession
val spark = SparkSession.builder
  .appName("My Spark Application")  // optional and will be autogenerated if not specified
  .master("local[*]")               // only for demo and testing purposes, use spark-submit instead
  .enableHiveSupport()              // self-explanatory, isn't it?
  .config("spark.sql.warehouse.dir", "target/spark-warehouse")
  .withExtensions { extensions =>
    extensions.injectResolutionRule { session =>
      ...
    }
    extensions.injectOptimizerRule { session =>
      ...
    }
  }
  .getOrCreate
```

!!! note "SparkSession in spark-shell"
    `spark` object in `spark-shell` (the instance of `SparkSession` that is auto-created) has Hive support enabled.

    In order to disable the pre-configured Hive support in the `spark` object, use [spark.sql.catalogImplementation](spark-sql-StaticSQLConf.md#spark.sql.catalogImplementation) internal configuration property with `in-memory` value (that uses [InMemoryCatalog](spark-sql-InMemoryCatalog.md) external catalog instead).

    ```text
    $ spark-shell --conf spark.sql.catalogImplementation=in-memory
    ```

There could be many `SparkSessions` in a single Spark application. The common use case is to keep relational entities separate logically in [catalogs](#catalog) per `SparkSession`.

## <span id="sessionState" /> SessionState

```scala
sessionState: SessionState
```

`sessionState` is the current [SessionState](spark-sql-SessionState.md).

Internally, `sessionState` <<spark-sql-SessionState.adoc#clone, clones>> the optional <<parentSessionState, parent SessionState>> (if given when <<creating-instance, creating the SparkSession>>) or <<instantiateSessionState, creates a new SessionState>> using <<BaseSessionStateBuilder.md#, BaseSessionStateBuilder>> as defined by <<spark-sql-StaticSQLConf.adoc#spark.sql.catalogImplementation, spark.sql.catalogImplementation>> configuration property:

* *in-memory* (default) for link:spark-sql-SessionStateBuilder.adoc[org.apache.spark.sql.internal.SessionStateBuilder]
* *hive* for link:hive/HiveSessionStateBuilder.adoc[org.apache.spark.sql.hive.HiveSessionStateBuilder]

## Creating Instance

`SparkSession` takes the following to be created:

* <span id="sparkContext"> `SparkContext`
* <span id="existingSharedState"> Optional existing [SharedState](spark-sql-SharedState.md)
* <span id="parentSessionState"> Optional parent [SessionState](spark-sql-SessionState.md)
* <span id="extensions"> [SparkSessionExtensions](spark-sql-SparkSessionExtensions.md)

`SparkSession` is created when:

* `SparkSession.Builder` is requested to [getOrCreate](spark-sql-SparkSession-Builder.md#getOrCreate)
* Indirectly using [newSession](#newSession) or [cloneSession](#cloneSession)

## <span id="newSession"> Creating New SparkSession

```scala
newSession(): SparkSession
```

`newSession` creates (starts) a new `SparkSession` (with the current `SparkContext` and [SharedState](spark-sql-SharedState.md)).

```text
scala> val newSession = spark.newSession
newSession: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@122f58a
```

## <span id="cloneSession"> Cloning SparkSession

```scala
cloneSession(): SparkSession
```

`cloneSession`...FIXME

!!! note "cloneSession VS newSession"
    How is cloneSession different from newSession? What's shared?

`cloneSession` is used when:

* `AdaptiveSparkPlanHelper` is requested to `getOrCloneSessionWithAqeOff`
* `StreamExecution` (Spark Structured Streaming) is created

## <span id="builder"> Creating SparkSession Using Builder Pattern

```scala
builder(): Builder
```

`builder` is an object method that creates a new [Builder](spark-sql-SparkSession-Builder.md) to build a `SparkSession` using a _fluent API_.

```scala
import org.apache.spark.sql.SparkSession
val builder = SparkSession.builder
```

TIP: Read about https://en.wikipedia.org/wiki/Fluent_interface[Fluent interface] design pattern in Wikipedia, the free encyclopedia.

## <span id="version"> Spark Version

```scala
version: String
```

`version` returns the version of Apache Spark in use.

Internally, `version` uses `spark.SPARK_VERSION` value that is the `version` property in `spark-version-info.properties` properties file on CLASSPATH.

## <span id="emptyDataset"> Creating Empty Dataset (Given Encoder)

```scala
emptyDataset[T: Encoder]: Dataset[T]
```

`emptyDataset` creates an empty [Dataset](spark-sql-Dataset.md) (assuming that future records being of type `T`).

```text
scala> val strings = spark.emptyDataset[String]
strings: org.apache.spark.sql.Dataset[String] = [value: string]

scala> strings.printSchema
root
 |-- value: string (nullable = true)
```

`emptyDataset` creates a [LocalRelation](logical-operators/LocalRelation.md) logical operator.

## <span id="createDataset"> Creating Dataset from Local Collections or RDDs

```scala
createDataset[T : Encoder](
  data: RDD[T]): Dataset[T]
createDataset[T : Encoder](
  data: Seq[T]): Dataset[T]
```

`createDataset` creates a [Dataset](spark-sql-Dataset.md) from a local Scala collection, i.e. `Seq[T]`, Java's `List[T]`, or a distributed `RDD[T]`.

```text
scala> val one = spark.createDataset(Seq(1))
one: org.apache.spark.sql.Dataset[Int] = [value: int]

scala> one.show
+-----+
|value|
+-----+
|    1|
+-----+
```

`createDataset` creates logical operators:

* [LocalRelation](logical-operators/LocalRelation.md) for the input `data` collection
* [LogicalRDD](logical-operators/LogicalRDD.md) for the input `RDD[T]`


!!! tip "implicits object"
    You may want to consider [implicits](spark-sql-SparkSession-implicits.md) object and `toDS` method instead.

    ```text
    val spark: SparkSession = ...
    import spark.implicits._

    scala> val one = Seq(1).toDS
    one: org.apache.spark.sql.Dataset[Int] = [value: int]
    ```

Internally, `createDataset` first looks up the implicit [ExpressionEncoder](spark-sql-ExpressionEncoder.md) in scope to access the ``AttributeReference``s (of the [schema](spark-sql-schema.md)).

The expression encoder is then used to map elements (of the input `Seq[T]`) into a collection of link:spark-sql-InternalRow.adoc[InternalRows]. With the references and rows, `createDataset` returns a link:spark-sql-Dataset.adoc[Dataset] with a link:spark-sql-LogicalPlan-LocalRelation.adoc[`LocalRelation` logical query plan].

## <span id="range"> Creating Dataset With Single Long Column

```scala
range(end: Long): Dataset[java.lang.Long]
range(start: Long, end: Long): Dataset[java.lang.Long]
range(start: Long, end: Long, step: Long): Dataset[java.lang.Long]
range(start: Long, end: Long, step: Long, numPartitions: Int): Dataset[java.lang.Long]
```

`range` method family create a [Dataset](spark-sql-Dataset.md) of `Long` numbers.

```text
scala> spark.range(start = 0, end = 4, step = 2, numPartitions = 5).show
+---+
| id|
+---+
|  0|
|  2|
+---+
```

The three first variants (that do not specify `numPartitions` explicitly) use `SparkContext.defaultParallelism` for the number of partitions.

Internally, `range` creates a new `Dataset[Long]` with [Range](logical-operators/Range.md) leaf logical operator and `Encoders.LONG` encoder.

## <span id="sql"> Executing SQL Queries (aka SQL Mode)

```scala
sql(sqlText: String): DataFrame
```

`sql` executes the `sqlText` SQL statement and creates a [DataFrame](spark-sql-DataFrame.md).

!!! note spark-shell
    `sql` is imported in `spark-shell` so you can execute SQL statements as if `sql` were a part of the environment.

    ```text
    scala> :imports
    1) import spark.implicits._       (72 terms, 43 are implicit)
    2) import spark.sql               (1 terms)
    ```

```
scala> sql("SHOW TABLES")
res0: org.apache.spark.sql.DataFrame = [tableName: string, isTemporary: boolean]

scala> sql("DROP TABLE IF EXISTS testData")
res1: org.apache.spark.sql.DataFrame = []

// Let's create a table to SHOW it
spark.range(10).write.option("path", "/tmp/test").saveAsTable("testData")

scala> sql("SHOW TABLES").show
+---------+-----------+
|tableName|isTemporary|
+---------+-----------+
| testdata|      false|
+---------+-----------+
```

Internally, `sql` requests the link:spark-sql-SessionState.adoc#sqlParser[current `ParserInterface`] to link:spark-sql-ParserInterface.adoc#parsePlan[execute a SQL query] that gives a link:spark-sql-LogicalPlan.adoc[LogicalPlan].

NOTE: `sql` uses `SessionState` link:spark-sql-SessionState.adoc#sqlParser[to access the current `ParserInterface`].

`sql` then creates a [DataFrame](spark-sql-DataFrame.md) using the current `SparkSession` (itself) and the [LogicalPlan](logical-operators/LogicalPlan.md).

!!! tip "spark-sql Command-Line Tool"
    Use [spark-sql](tools/spark-sql-spark-sql.md) command-line tool to use SQL directly (not Scala as in `spark-shell`).

    ```text
    spark-sql> show databases;
    default
    Time taken: 0.028 seconds, Fetched 1 row(s)
    ```

## <span id="udf"> Accessing UDFRegistration

```scala
udf: UDFRegistration
```

`udf` attribute is [UDFRegistration](spark-sql-UDFRegistration.md) (for registering [user-defined functions](spark-sql-udfs.md) for SQL-based queries).

```text
val spark: SparkSession = ...
spark.udf.register("myUpper", (s: String) => s.toUpperCase)

val strs = ('a' to 'c').map(_.toString).toDS
strs.registerTempTable("strs")

scala> sql("SELECT *, myUpper(value) UPPER FROM strs").show
+-----+-----+
|value|UPPER|
+-----+-----+
|    a|    A|
|    b|    B|
|    c|    C|
+-----+-----+
```

Internally, it is simply an alias for [SessionState.udfRegistration](spark-sql-SessionState.md#udfRegistration).

## <span id="table"> Loading Data From Table

```scala
table(
  multipartIdentifier: Seq[String]): DataFrame
table(
  tableName: String): DataFrame
table(
  tableIdent: TableIdentifier): DataFrame
```

`table` creates a [DataFrame](spark-sql-DataFrame.md) for the input `tableName` table.

!!! note
    [baseRelationToDataFrame](#baseRelationToDataFrame) acts as a mechanism to plug `BaseRelation` object hierarchy in into link:adoc[LogicalPlan](logical-operators/LogicalPlan.md) object hierarchy that `SparkSession` uses to bridge them.

```text
scala> spark.catalog.tableExists("t1")
res1: Boolean = true

// t1 exists in the catalog
// let's load it
val t1 = spark.table("t1")
```

## <span id="catalog"> Metadata Catalog

```scala
catalog: Catalog
```

`catalog` attribute is a (lazy) interface to the [metadata catalog](spark-sql-Catalog.md) (of relational entities like databases, tables, functions, table columns, and views).

```text
scala> spark.catalog.listTables.show
+------------------+--------+-----------+---------+-----------+
|              name|database|description|tableType|isTemporary|
+------------------+--------+-----------+---------+-----------+
|my_permanent_table| default|       null|  MANAGED|      false|
|              strs|    null|       null|TEMPORARY|       true|
+------------------+--------+-----------+---------+-----------+
```

Internally, `catalog` creates a [CatalogImpl](spark-sql-CatalogImpl.md) (that uses the current `SparkSession`).

## <span id="read"> DataFrameReader

```scala
read: DataFrameReader
```

`read` gives [DataFrameReader](DataFrameReader.md) to load data from external data sources and load it into a `DataFrame`.

```scala
val spark: SparkSession = ... // create instance
val dfReader: DataFrameReader = spark.read
```

## <span id="conf"> Runtime Configuration

```scala
conf: RuntimeConfig
```

`conf` returns the current [RuntimeConfig](spark-sql-RuntimeConfig.md).

Internally, `conf` creates a <<spark-sql-RuntimeConfig.adoc#creating-instance, RuntimeConfig>> (when requested the very first time and cached afterwards) with the <<spark-sql-SessionState.adoc#conf, SQLConf>> of the <<sessionState, SessionState>>.

## <span id="experimentalMethods"> ExperimentalMethods

```scala
experimental: ExperimentalMethods
```

`experimentalMethods` is an extension point with [ExperimentalMethods](spark-sql-ExperimentalMethods.md) that is a per-session collection of extra strategies and ``Rule[LogicalPlan]``s.

`experimental` is used in [SparkPlanner](spark-sql-SparkPlanner.md) and [SparkOptimizer](spark-sql-SparkOptimizer.md).

## <span id="baseRelationToDataFrame"> Create DataFrame for BaseRelation

```scala
baseRelationToDataFrame(
  baseRelation: BaseRelation): DataFrame
```

Internally, `baseRelationToDataFrame` creates a [DataFrame](spark-sql-DataFrame.md) from the input [BaseRelation](spark-sql-BaseRelation.md) wrapped inside [LogicalRelation](logical-operators/LogicalRelation.md).

`baseRelationToDataFrame` is used when:

* `DataFrameReader` is requested to load data from [data source](DataFrameReader.md#load) or [JDBC table](DataFrameReader.md#jdbc)
* `TextInputCSVDataSource` creates a base `Dataset` (of Strings)
* `TextInputJsonDataSource` creates a base `Dataset` (of Strings)

## <span id="instantiateSessionState"> Creating SessionState

```scala
instantiateSessionState(
  className: String,
  sparkSession: SparkSession): SessionState
```

`instantiateSessionState` finds the `className` that is then used to [create](BaseSessionStateBuilder.md#creating-instance) and [build](BaseSessionStateBuilder.md#build) a `BaseSessionStateBuilder`.

`instantiateSessionState` may report an `IllegalArgumentException` while instantiating the class of a `SessionState`:

```text
Error while instantiating '[className]'
```

`instantiateSessionState` is used when `SparkSession` is requested for [SessionState](#sessionState) (based on [spark.sql.catalogImplementation](spark-sql-StaticSQLConf.md#spark.sql.catalogImplementation) configuration property).

## <span id="sessionStateClassName"> sessionStateClassName

```scala
sessionStateClassName(
  conf: SparkConf): String
```

`sessionStateClassName` gives the name of the class of the [SessionState](spark-sql-SessionState.md) per [spark.sql.catalogImplementation](spark-sql-StaticSQLConf.md#spark.sql.catalogImplementation), i.e.

* [org.apache.spark.sql.hive.HiveSessionStateBuilder](hive/HiveSessionStateBuilder.md) for `hive`
* [org.apache.spark.sql.internal.SessionStateBuilder](spark-sql-SessionStateBuilder.md) for `in-memory`

`sessionStateClassName` is used when `SparkSession` is requested for the [SessionState](#sessionState) (and one is not available yet).

## <span id="internalCreateDataFrame"> Creating DataFrame From RDD Of Internal Binary Rows and Schema

```scala
internalCreateDataFrame(
  catalystRows: RDD[InternalRow],
  schema: StructType,
  isStreaming: Boolean = false): DataFrame
```

`internalCreateDataFrame` creates a [DataFrame](spark-sql-Dataset.md#ofRows) with [LogicalRDD](logical-operators/LogicalRDD.md).

`internalCreateDataFrame` is used when:

* `DataFrameReader` is requested to create a DataFrame from Dataset of [JSONs](DataFrameReader.md#json) or [CSVs](DataFrameReader.md#csv)

* `SparkSession` is requested to [create a DataFrame from RDD of rows](#createDataFrame)

* [InsertIntoDataSourceCommand](logical-operators/InsertIntoDataSourceCommand.md) logical command is executed

## <span id="listenerManager"> ExecutionListenerManager

```scala
listenerManager: ExecutionListenerManager
```

`listenerManager` is the [ExecutionListenerManager](spark-sql-ExecutionListenerManager.md)

## <span id="sharedState"> SharedState

```scala
sharedState: SharedState
```

`sharedState` is the [SharedState](spark-sql-SharedState.md)

## <span id="time"> Measuring Duration of Executing Code Block

```scala
time[T](f: => T): T
```

`time` executes a code block and prints out (to standard output) the time taken to execute it

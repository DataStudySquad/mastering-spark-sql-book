# SparkSession.Builder

`SparkSession.Builder` is a [builder interface](#methods) to create [SparkSession](SparkSession.md)s.

## Accessing Builder

`Builder` is available using the [SparkSession.builder](.md#builder) factory method.

```scala
import org.apache.spark.sql.SparkSession
val spark = SparkSession.builder
  .appName("My Spark Application")  // optional and will be autogenerated if not specified
  .master("local[*]")               // only for demo and testing purposes, use spark-submit instead
  .enableHiveSupport()              // self-explanatory, isn't it?
  .config("spark.sql.warehouse.dir", "target/spark-warehouse")
  .withExtensions { extensions =>
    extensions.injectResolutionRule { session =>
      ...
    }
    extensions.injectOptimizerRule { session =>
      ...
    }
  }
  .getOrCreate
```

## <span id="enableHiveSupport"> Enabling Hive Support

```scala
enableHiveSupport(): Builder
```

`enableHiveSupport` enables [Hive support](hive/index.md).

!!! note
    You do *not* need any existing Hive installation to use Spark's Hive support. `SparkSession` context will automatically create `metastore_db` in the current directory of a Spark application and the directory configured by [spark.sql.warehouse.dir](spark-sql-StaticSQLConf.md#spark.sql.warehouse.dir) configuration property.

    Consult [SharedState](spark-sql-SharedState.md).

Internally, `enableHiveSupport` checks whether [Hive classes are available or not](#hiveClassesArePresent). If so, `enableHiveSupport` sets [spark.sql.catalogImplementation](spark-sql-StaticSQLConf.md#spark.sql.catalogImplementation) internal configuration property to `hive`. Otherwise, `enableHiveSupport` throws an `IllegalArgumentException`:

```text
Unable to instantiate SparkSession with Hive support because Hive classes are not found.
```

## <span id="getOrCreate"> Getting Or Creating SparkSession Instance

```scala
getOrCreate(): SparkSession
```

`getOrCreate` gives the active [SparkSession](SparkSession.md) or creates a new one.

While creating a new one, `getOrCreate` finds the SparkSession extensions (based on [spark.sql.extensions](spark-sql-StaticSQLConf.md#spark.sql.extensions) configuration property) and [applies them](SparkSession.md#applyExtensions) to the [SparkSessionExtensions](#extensions).

## <span id="extensions"> SparkSessionExtensions

```scala
extensions: SparkSessionExtensions
```

`Builder` creates a new [SparkSessionExtensions](spark-sql-SparkSessionExtensions.md) when [created](#creating-instance).

The `SparkSessionExtensions` is used to [apply SparkSession extensions](SparkSession.md#applyExtensions) registered using [spark.sql.extensions](spark-sql-StaticSQLConf.md#SPARK_SESSION_EXTENSIONS) configuration property or [Builder.withExtensions](#withExtensions) method.

In the end, `Builder` uses the `SparkSessionExtensions` to [create a new SparkSession](#getOrCreate).

## <span id="withExtensions"> Registering SparkSessionExtensions

```scala
withExtensions(
  f: SparkSessionExtensions => Unit): Builder
```

Allows registering SparkSession extensions using [SparkSessionExtensions](spark-sql-SparkSessionExtensions.md)

`withExtensions` simply executes the input `f` function with a `SparkSessionExtensions`.

## <span id="hiveClassesArePresent"> hiveClassesArePresent

```scala
hiveClassesArePresent: Boolean
```

`hiveClassesArePresent` loads and initializes [org.apache.spark.sql.hive.HiveSessionStateBuilder](hive/HiveSessionStateBuilder.md) and `org.apache.hadoop.hive.conf.HiveConf` classes from the current classloader.

`hiveClassesArePresent` returns `true` when the initialization succeeded, and `false` otherwise (due to `ClassNotFoundException` or `NoClassDefFoundError` errors).

`hiveClassesArePresent` is used when:

* `Builder` is requested to [enableHiveSupport](#enableHiveSupport)

* `spark-shell` is executed

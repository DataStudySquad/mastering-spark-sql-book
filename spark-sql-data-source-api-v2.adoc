== Data Source API V2

*Data Source API V2* (_DataSource API V2_ or _DataSource V2_) is a new API for data sources in Spark SQL with the following abstractions (_contracts_):

* <<spark-sql-DataSourceV2.adoc#, DataSourceV2>>

* <<spark-sql-ReadSupport.adoc#, ReadSupport>>

* <<spark-sql-DataSourceReader.adoc#, DataSourceReader>>

* <<spark-sql-WriteSupport.adoc#, WriteSupport>>

* <<spark-sql-DataSourceWriter.adoc#, DataSourceWriter>>

* <<spark-sql-SessionConfigSupport.adoc#, SessionConfigSupport>>

* <<spark-sql-DataSourceV2StringFormat.adoc#, DataSourceV2StringFormat>>

* <<spark-sql-InputPartition.adoc#, InputPartition>>

NOTE: The work on Data Source API V2 was tracked under https://issues.apache.org/jira/browse/SPARK-15689[SPARK-15689 Data source API v2] that was fixed in Apache Spark 2.3.0.

NOTE: Data Source API V2 is already heavily used in Spark Structured Streaming.

=== Query Planning and Execution

Data Source API V2 relies on the <<spark-sql-SparkStrategy-DataSourceV2Strategy.adoc#, DataSourceV2Strategy>> execution planning strategy for query planning.

Among the logical operators handled is <<spark-sql-LogicalPlan-DataSourceV2Relation.adoc#, DataSourceV2Relation>> logical operator that is _translated_ to a <<spark-sql-SparkPlan-DataSourceV2ScanExec.adoc#, DataSourceV2ScanExec>> physical operator.

When <<spark-sql-SparkPlan-DataSourceV2ScanExec.adoc#doExecute, executed>>, `DataSourceV2ScanExec` physical operator creates a <<spark-sql-DataSourceRDD.adoc#, DataSourceRDD>> (or a `ContinuousReader` for Spark Structured Streaming).

`DataSourceRDD` uses <<spark-sql-InputPartition.adoc#, InputPartitions>> for <<spark-sql-DataSourceRDD.adoc#getPartitions, partitions>>, <<spark-sql-DataSourceRDD.adoc#getPreferredLocations, preferred locations>>, and <<spark-sql-DataSourceRDD.adoc#compute, computing partitions>>.

=== [[filter-pushdown]] Filter Pushdown Performance Optimization

Data Source API V2 supports *filter pushdown* performance optimization for <<spark-sql-DataSourceReader.adoc#, DataSourceReaders>> with <<spark-sql-SupportsPushDownFilters.adoc#, SupportsPushDownFilters>> (that is applied when <<spark-sql-SparkStrategy-DataSourceV2Strategy.adoc#, DataSourceV2Strategy>> execution planning strategy is requested to plan a <<spark-sql-SparkStrategy-DataSourceV2Strategy.adoc#apply-DataSourceV2Relation, DataSourceV2Relation>> logical operator).

(From https://drill.apache.org/docs/parquet-filter-pushdown/[Parquet Filter Pushdown] in Apache Drill's documentation) Filter pushdown is a performance optimization that prunes extraneous data while reading from a data source to reduce the amount of data to scan and read for queries with <<spark-sql-SparkStrategy-DataSourceStrategy.adoc#translateFilter, supported filter expressions>>. Pruning data reduces the I/O, CPU, and network overhead to optimize query performance.

TIP: Enable INFO logging level for the <<spark-sql-SparkStrategy-DataSourceV2Strategy.adoc#logging, DataSourceV2Strategy logger>> to be told <<spark-sql-SparkStrategy-DataSourceV2Strategy.adoc#apply-DataSourceV2Relation, what the pushed filters are>>.

=== [[i-want-more]] Further Reading and Watching

. (video) https://databricks.com/session/apache-spark-data-source-v2[Apache Spark Data Source V2 by Wenchen Fan and Gengliang Wang]

== [[VectorizedParquetRecordReader]] VectorizedParquetRecordReader

`VectorizedParquetRecordReader` is a link:spark-sql-SpecificParquetRecordReaderBase.adoc[SpecificParquetRecordReaderBase] for *parquet* file format that directly materialize to Java `Objects`.

`VectorizedParquetRecordReader` is <<creating-instance, created>> exclusively when `ParquetFileFormat` is requested to link:spark-sql-ParquetFileFormat.adoc#buildReaderWithPartitionValues[build a data reader with partition column values appended] (when link:spark-sql-properties.adoc#spark.sql.parquet.enableVectorizedReader[spark.sql.parquet.enableVectorizedReader] configuration property is enabled and the result schema uses link:spark-sql-DataType.adoc#AtomicType[AtomicType] data types only).

[NOTE]
====
link:spark-sql-properties.adoc#spark.sql.parquet.enableVectorizedReader[spark.sql.parquet.enableVectorizedReader] configuration property is enabled (`true`) by default.

[source, scala]
----
val isParquetVectorizedReaderEnabled = spark.conf.get("spark.sql.parquet.enableVectorizedReader").toBoolean
assert(isParquetVectorizedReaderEnabled, "spark.sql.parquet.enableVectorizedReader should be enabled by default")
----
====

`VectorizedParquetRecordReader` uses `OFF_HEAP` <<MEMORY_MODE, memory mode>> when link:spark-sql-properties.adoc#spark.sql.columnVector.offheap.enabled[spark.sql.columnVector.offheap.enabled] internal configuration property is enabled (which is not by default).

[[CAPACITY]]
`VectorizedParquetRecordReader` uses `4 * 1024` for capacity.

[[internal-registries]]
.VectorizedParquetRecordReader's Internal Properties (e.g. Registries, Counters and Flags)
[cols="1,2",options="header",width="100%"]
|===
| Name
| Description

| [[columnarBatch]] `columnarBatch`
| link:spark-sql-ColumnarBatch.adoc[ColumnarBatch]

| [[columnVectors]] `columnVectors`
| Allocated `WritableColumnVectors`

| [[MEMORY_MODE]] `MEMORY_MODE`
a| Memory mode of the <<columnarBatch, ColumnarBatch>>

* [[OFF_HEAP]] `OFF_HEAP` (when <<useOffHeap, useOffHeap>> is on as per link:spark-sql-properties.adoc#spark.sql.columnVector.offheap.enabled[spark.sql.columnVector.offheap.enabled] configuration property)
* [[ON_HEAP]] `ON_HEAP`

Used exclusively when `VectorizedParquetRecordReader` is requested to <<initBatch, initBatch>>.

| [[missingColumns]] `missingColumns`
| Bitmap of columns (per index) that are missing (or simply the ones that the reader should not read)
|===

=== [[nextKeyValue]] `nextKeyValue` Method

[source, java]
----
boolean nextKeyValue() throws IOException
----

NOTE: `nextKeyValue` is part of Hadoop's https://hadoop.apache.org/docs/r2.7.4/api/org/apache/hadoop/mapred/RecordReader.html[RecordReader] to read (key, value) pairs from a Hadoop https://hadoop.apache.org/docs/r2.7.4/api/org/apache/hadoop/mapred/InputSplit.html[InputSplit] to present a record-oriented view.

`nextKeyValue`...FIXME

NOTE: `nextKeyValue` is used when...FIXME

=== [[resultBatch]] `resultBatch` Method

[source, java]
----
ColumnarBatch resultBatch()
----

`resultBatch` gives <<columnarBatch, columnarBatch>> if available or does <<initBatch, initBatch>>.

NOTE: `resultBatch` is used exclusively when `VectorizedParquetRecordReader` is requested to <<nextKeyValue, nextKeyValue>>.

=== [[creating-instance]] Creating VectorizedParquetRecordReader Instance

`VectorizedParquetRecordReader` takes the following when created:

* [[convertTz]] `TimeZone` (`null` when no timezone conversion is expected)
* [[useOffHeap]] `useOffHeap` flag (per link:spark-sql-properties.adoc#spark.sql.columnVector.offheap.enabled[spark.sql.columnVector.offheap.enabled] configuration property)

`VectorizedParquetRecordReader` initializes the <<internal-registries, internal registries and counters>>.

=== [[initialize]] `initialize` Method

[source, java]
----
void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)
----

NOTE: `initialize` is part of link:spark-sql-SpecificParquetRecordReaderBase.adoc#initialize[SpecificParquetRecordReaderBase Contract] to...FIXME.

`initialize`...FIXME

=== [[enableReturningBatches]] `enableReturningBatches` Method

[source, java]
----
void enableReturningBatches()
----

`enableReturningBatches`...FIXME

NOTE: `enableReturningBatches` is used when...FIXME

=== [[initBatch]] `initBatch` Method

[source, java]
----
void initBatch(StructType partitionColumns, InternalRow partitionValues) // <1>
// private
private void initBatch() // <2>
private void initBatch(
  MemoryMode memMode,
  StructType partitionColumns,
  InternalRow partitionValues)
----
<1> Uses <<MEMORY_MODE, MEMORY_MODE>>
<2> Uses <<MEMORY_MODE, MEMORY_MODE>> and no `partitionColumns` and no `partitionValues`

`initBatch` creates the batch link:spark-sql-schema.adoc[schema] that is link:spark-sql-SpecificParquetRecordReaderBase.adoc#sparkSchema[sparkSchema] and the input `partitionColumns` schema.

`initBatch` requests link:spark-sql-OffHeapColumnVector.adoc#allocateColumns[OffHeapColumnVector] or link:spark-sql-OnHeapColumnVector.adoc#allocateColumns[OnHeapColumnVector] to allocate column vectors per the input `memMode`, i.e. <<OFF_HEAP, OFF_HEAP>> or <<ON_HEAP, ON_HEAP>> memory modes, respectively. `initBatch` records the allocated column vectors as the internal <<columnVectors, WritableColumnVectors>>.

[NOTE]
====
link:spark-sql-properties.adoc#spark.sql.columnVector.offheap.enabled[spark.sql.columnVector.offheap.enabled] configuration property controls <<OFF_HEAP, OFF_HEAP>> or <<ON_HEAP, ON_HEAP>> memory modes, i.e. `true` or `false`, respectively.

`spark.sql.columnVector.offheap.enabled` is disabled by default which means that link:spark-sql-OnHeapColumnVector.adoc[OnHeapColumnVector] is used.
====

`initBatch` creates a link:spark-sql-ColumnarBatch.adoc#creating-instance[ColumnarBatch] (with the <<columnVectors, allocated WritableColumnVectors>>) and records it as the internal <<columnarBatch, ColumnarBatch>>.

`initBatch` creates new slots in the <<columnVectors, allocated WritableColumnVectors>> for the input `partitionColumns` and sets the input `partitionValues` as constants.

`initBatch` initializes <<missingColumns, missing columns>> with `nulls`.

[NOTE]
====
`initBatch` is used when:

* `VectorizedParquetRecordReader` is requested for <<resultBatch, resultBatch>>

* `ParquetFileFormat` is requested to link:spark-sql-ParquetFileFormat.adoc#buildReaderWithPartitionValues[build a data reader with partition column values appended]
====

== [[DataSourceRDD]] DataSourceRDD -- Input RDD Of DataSourceV2ScanExec Physical Operator

`DataSourceRDD` is an `RDD` that is <<creating-instance, created>> exclusively when `DataSourceV2ScanExec` physical operator is requested for the link:spark-sql-SparkPlan-DataSourceV2ScanExec.adoc#inputRDD[input RDD] (when `WholeStageCodegenExec` physical operator is link:spark-sql-SparkPlan-WholeStageCodegenExec.adoc#doExecute[executed]).

`DataSourceRDD` uses link:spark-sql-DataSourceRDDPartition.adoc[DataSourceRDDPartition] partitions.

[[creating-instance]]
`DataSourceRDD` takes the following to be created:

* [[sc]] `SparkContext`
* [[inputPartitions]] <<spark-sql-InputPartition.adoc#, InputPartitions>> (`Seq[InputPartition[T]]`)

=== [[getPreferredLocations]] Requesting Preferred Locations (For Partition) -- `getPreferredLocations` Method

[source, scala]
----
getPreferredLocations(split: Partition): Seq[String]
----

NOTE: `getPreferredLocations` is part of Spark Core's `RDD` Contract to...FIXME.

`getPreferredLocations`...FIXME

=== [[getPartitions]] `getPartitions` Method

[source, scala]
----
getPartitions: Array[Partition]
----

NOTE: `getPartitions` is part of Spark Core's `RDD` Contract to...FIXME

`getPartitions`...FIXME

=== [[compute]] Computing Partition (in TaskContext) -- `compute` Method

[source, scala]
----
compute(split: Partition, context: TaskContext): Iterator[T]
----

NOTE: `compute` is part of Spark Core's `RDD` Contract to compute a partition (in a `TaskContext`).

`compute`...FIXME

`compute` registers a Spark Core `TaskCompletionListener` that requests the `DataReader` to close at a task completion.

`compute` returns a Spark Core `InterruptibleIterator` that...FIXME

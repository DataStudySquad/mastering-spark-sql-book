== Configuration Properties

This page contains the link:../spark-sql-properties.adoc[configuration properties] of the link:index.adoc[Hive data source].

[[properties]]
.Spark SQL Configuration Properties (spark.sql.hive)
[cols="1m,2",options="header",width="100%"]
|===
| Name
| Description

| s.s.h.convertMetastoreOrc
a| [[spark.sql.hive.convertMetastoreOrc]] When enabled (i.e. `true`), the built-in ORC reader and writer are used to process ORC tables created by using the HiveQL syntax (instead of Hive serde).

Default: `true`

Use link:spark-sql-SQLConf.adoc#CONVERT_METASTORE_ORC[SQLConf.CONVERT_METASTORE_ORC] to reference the property

| s.s.h.convertMetastoreParquet
a| [[spark.sql.hive.convertMetastoreParquet]] Controls whether to use the built-in Parquet reader and writer to process parquet tables created by using the HiveQL syntax (instead of Hive serde).

Default: `true`

Use link:spark-sql-SQLConf.adoc#CONVERT_METASTORE_PARQUET[SQLConf.CONVERT_METASTORE_PARQUET] to reference the property

| s.s.h.convertMetastoreParquet.mergeSchema
a| [[spark.sql.hive.convertMetastoreParquet.mergeSchema]] Enables trying to merge possibly different but compatible Parquet schemas in different Parquet data files.

Default: `false`

This configuration is only effective when <<spark.sql.hive.convertMetastoreParquet, spark.sql.hive.convertMetastoreParquet>> is enabled.

| s.s.h.manageFilesourcePartitions
a| [[spark.sql.hive.manageFilesourcePartitions]] Enables *metastore partition management* for file source tables (_filesource partition management_). This includes both datasource and converted Hive tables.

Default: `true`

When enabled (`true`), datasource tables store partition in the Hive metastore, and use the metastore to prune partitions during query planning.

Use link:spark-sql-SQLConf.adoc#manageFilesourcePartitions[SQLConf.manageFilesourcePartitions] method to access the current value.

| s.s.h.metastore.barrierPrefixes
a| [[spark.sql.hive.metastore.barrierPrefixes]] Comma-separated list of class prefixes that should explicitly be reloaded for each version of Hive that Spark SQL is communicating with, e.g. Hive UDFs that are declared in a prefix that typically would be shared (i.e. `org.apache.spark.*`)

Default: `(empty)`

| s.s.h.metastore.jars
a| [[spark.sql.hive.metastore.jars]] Location of the jars that should be used to link:HiveUtils.adoc#newClientForMetadata[create a HiveClientImpl].

Default: `builtin`

Supported locations:

* `builtin` - the jars that were used to load Spark SQL (aka _Spark classes_). Valid only when using the execution version of Hive, i.e. <<spark.sql.hive.metastore.version, spark.sql.hive.metastore.version>>

* `maven` - download the Hive jars from Maven repositories

* Classpath in the standard format for both Hive and Hadoop

| s.s.h.metastore.sharedPrefixes
a| [[spark.sql.hive.metastore.sharedPrefixes]] Comma-separated list of class prefixes that should be loaded using the classloader that is shared between Spark SQL and a specific version of Hive.

Default: `"com.mysql.jdbc", "org.postgresql", "com.microsoft.sqlserver", "oracle.jdbc"`

An example of classes that should be shared are:

* JDBC drivers that are needed to talk to the metastore

* Other classes that interact with classes that are already shared, e.g. custom appenders that are used by log4j

| s.s.h.metastore.version
a| [[spark.sql.hive.metastore.version]] Version of the Hive metastore (and the link:HiveUtils.adoc#newClientForMetadata[client classes and jars]).

Default: link:HiveUtils.adoc#builtinHiveVersion[1.2.1]

Supported versions link:IsolatedClientLoader.adoc#hiveVersion[range from `0.12.0` up to and including `2.3.3`]

| s.s.h.verifyPartitionPath
a| [[spark.sql.hive.verifyPartitionPath]] When enabled (`true`), check all the partition paths under the table's root directory when reading data stored in HDFS. This configuration will be deprecated in the future releases and replaced by spark.files.ignoreMissingFiles.

Default: `false`

| s.s.h.metastorePartitionPruning
a| [[spark.sql.hive.metastorePartitionPruning]] When enabled (`true`), some predicates will be pushed down into the Hive metastore so that unmatching partitions can be eliminated earlier.

Default: `true`

This only affects Hive tables not converted to filesource relations (see HiveUtils.CONVERT_METASTORE_PARQUET and HiveUtils.CONVERT_METASTORE_ORC for more information.

Use link:spark-sql-SQLConf.adoc#metastorePartitionPruning[SQLConf.metastorePartitionPruning] method to access the current value.

| s.s.h.filesourcePartitionFileCacheSize
a| [[spark.sql.hive.filesourcePartitionFileCacheSize]]

| s.s.h.caseSensitiveInferenceMode
a| [[spark.sql.hive.caseSensitiveInferenceMode]]

| s.s.h.convertCTAS
a| [[spark.sql.hive.convertCTAS]]

| s.s.h.gatherFastStats
a| [[spark.sql.hive.gatherFastStats]]

| s.s.h.advancedPartitionPredicatePushdown.enabled
a| [[spark.sql.hive.advancedPartitionPredicatePushdown.enabled]]

|===

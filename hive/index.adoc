== Hive Data Source

Spark SQL supports relational entities (tables, functions, etc.) managed by Apache Hive using *Hive data source*.

TIP: Consult link:../demo/demo-connecting-spark-sql-to-hive-metastore.adoc[Demo: Connecting Spark SQL to Hive Metastore (with Remote Metastore Server)] to learn in a more practical approach.

Every Spark SQL application uses a link:../spark-sql-SparkSession.adoc[SparkSession]. In order to use Hive-related features the `SparkSession` has to be created using link:../spark-sql-SparkSession-Builder.adoc#enableHiveSupport[Builder.enableHiveSupport] (that makes sure the link:../spark-sql-SparkSession-Builder.adoc#hiveClassesArePresent[Hive classes are available] and sets link:../spark-sql-StaticSQLConf.adoc#spark.sql.catalogImplementation[spark.sql.catalogImplementation] internal configuration property to `hive` when the required Hive classes are available).

Hive Data Source uses link:HiveSessionStateBuilder.adoc[HiveSessionStateBuilder] (to build a Hive-specific link:../spark-sql-SparkSession.adoc#sessionState[SessionState]) and link:HiveExternalCatalog.adoc[HiveExternalCatalog].

Hive Data Source uses custom link:configuration-properties.adoc[configuration properties].

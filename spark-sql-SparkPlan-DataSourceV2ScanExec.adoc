== [[DataSourceV2ScanExec]] DataSourceV2ScanExec Leaf Physical Operator

`DataSourceV2ScanExec` is a link:spark-sql-SparkPlan.adoc#LeafExecNode[leaf physical operator] to represent link:spark-sql-LogicalPlan-DataSourceV2Relation.adoc[DataSourceV2Relation] logical operators at execution time.

NOTE: A link:spark-sql-LogicalPlan-DataSourceV2Relation.adoc[DataSourceV2Relation] logical operator is created when...FIXME

`DataSourceV2ScanExec` is a link:spark-sql-ColumnarBatchScan.adoc[ColumnarBatchScan] that <<supportsBatch, supports vectorized batch decoding>> (when <<creating-instance, created>> for a <<reader, DataSourceReader>> that supports it, i.e. the `DataSourceReader` is a link:spark-sql-SupportsScanColumnarBatch.adoc[SupportsScanColumnarBatch] with the link:spark-sql-SupportsScanColumnarBatch.adoc#enableBatchRead[enableBatchRead] flag enabled).

`DataSourceV2ScanExec` is also a `DataSourceReaderHolder`.

`DataSourceV2ScanExec` is <<creating-instance, created>> exclusively when `DataSourceV2Strategy` execution planning strategy is link:spark-sql-SparkStrategy-DataSourceV2Strategy.adoc#apply[executed] and finds a link:spark-sql-LogicalPlan-DataSourceV2Relation.adoc[DataSourceV2Relation] logical operator in a logical query plan.

[[inputRDDs]]
`DataSourceV2ScanExec` gives the single <<inputRDD, input RDD>> as the link:spark-sql-CodegenSupport.adoc#inputRDDs[only input RDD of internal rows] (when `WholeStageCodegenExec` physical operator is link:spark-sql-SparkPlan-WholeStageCodegenExec.adoc#doExecute[executed]).

[[internal-registries]]
.DataSourceV2ScanExec's Internal Properties (e.g. Registries, Counters and Flags)
[cols="1m,3",options="header",width="100%"]
|===
| Name
| Description

| partitions
a| [[partitions]] Input partitions (`Seq[InputPartition[InternalRow]]`)

| readerFactories
a| [[readerFactories]] Collection of link:spark-sql-DataReaderFactory.adoc[DataReaderFactory] objects of link:spark-sql-UnsafeRow.adoc[UnsafeRows]

Used when...FIXME

|===

=== [[doExecute]] Executing Physical Operator (Generating RDD[InternalRow]) -- `doExecute` Method

[source, scala]
----
doExecute(): RDD[InternalRow]
----

NOTE: `doExecute` is part of <<spark-sql-SparkPlan.adoc#doExecute, SparkPlan Contract>> to generate the runtime representation of a structured query as a distributed computation over <<spark-sql-InternalRow.adoc#, internal binary rows>> on Apache Spark (i.e. `RDD[InternalRow]`).

`doExecute`...FIXME

=== [[supportsBatch]] `supportsBatch` Property

[source, scala]
----
supportsBatch: Boolean
----

NOTE: `supportsBatch` is part of link:spark-sql-ColumnarBatchScan.adoc#supportsBatch[ColumnarBatchScan Contract] to control whether the physical operator supports link:spark-sql-vectorized-parquet-reader.adoc[vectorized decoding] or not.

`supportsBatch` is enabled (i.e. `true`) only when the <<reader, DataSourceReader>> is a link:spark-sql-SupportsScanColumnarBatch.adoc[SupportsScanColumnarBatch] with the link:spark-sql-SupportsScanColumnarBatch.adoc#enableBatchRead[enableBatchRead] flag enabled.

NOTE: link:spark-sql-SupportsScanColumnarBatch.adoc#enableBatchRead[enableBatchRead] flag is enabled by default.

`supportsBatch` is disabled (i.e. `false`) otherwise.

=== [[creating-instance]] Creating DataSourceV2ScanExec Instance

`DataSourceV2ScanExec` takes the following when created:

* [[output]] Output schema (as a collection of `AttributeReferences`)
* [[reader]] link:spark-sql-DataSourceReader.adoc[DataSourceReader]

`DataSourceV2ScanExec` initializes the <<internal-registries, internal registries and counters>>.

=== [[inputRDD]] Creating Input RDD of Internal Rows -- `inputRDD` Internal Property

[source, scala]
----
inputRDD: RDD[InternalRow]
----

NOTE: `inputRDD` is a Scala lazy value which is computed once when accessed and cached afterwards.

`inputRDD`...FIXME

NOTE: `inputRDD` is used when `DataSourceV2ScanExec` physical operator is requested for the <<inputRDDs, input RDDs>> and to <<doExecute, execute>>.
